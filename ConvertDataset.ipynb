{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#OUTPUT_DIR_TRAIN='C:/Users/Admin/Desktop/deep_learning_data/colaborative_filtering/TFRecords_normal_ratings/tf_records_1M/train'\n",
    "#OUTPUT_DIR_TEST='C:/Users/Admin/Desktop/deep_learning_data/colaborative_filtering/TFRecords_normal_ratings/tf_records_1M/test'\n",
    "\n",
    "\n",
    "def _add_to_tfrecord(data_sample,tfrecord_writer):\n",
    "    \n",
    "    data_sample=list(data_sample.astype(dtype=np.float32))\n",
    "#     print (np.shape(data_sample))\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={'movie_ratings': float_feature(data_sample)}))                                          \n",
    "    tfrecord_writer.write(example.SerializeToString())\n",
    "    \n",
    "\n",
    "def _get_output_filename(output_dir, idx, name):\n",
    "    return '%s/%s_%03d.tfrecord' % (output_dir, name, idx)\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "am I even reachng?\n",
      "enabled\n",
      "9989\n",
      "10000\n",
      "converted\n",
      "1147\n",
      "C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_001.tfrecord\n",
      ">> Converting sample 100/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_002.tfrecord\n",
      ">> Converting sample 200/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_003.tfrecord\n",
      ">> Converting sample 300/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_004.tfrecord\n",
      ">> Converting sample 400/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_005.tfrecord\n",
      ">> Converting sample 500/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_006.tfrecord\n",
      ">> Converting sample 600/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_007.tfrecord\n",
      ">> Converting sample 700/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_008.tfrecord\n",
      ">> Converting sample 800/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_009.tfrecord\n",
      ">> Converting sample 900/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_010.tfrecord\n",
      ">> Converting sample 1000/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_011.tfrecord\n",
      ">> Converting sample 1100/1147C:\\Users\\nambi\\Desktop\\CF\\Project\\train/train_012.tfrecord\n",
      ">> Converting sample 1147/1147278\n",
      "C:\\Users\\nambi\\Desktop\\CF\\Project\\test/test_001.tfrecord\n",
      ">> Converting sample 100/278C:\\Users\\nambi\\Desktop\\CF\\Project\\test/test_002.tfrecord\n",
      ">> Converting sample 200/278C:\\Users\\nambi\\Desktop\\CF\\Project\\test/test_003.tfrecord\n",
      ">> Converting sample 278/278\n",
      "Finished converting the dataset!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ''' Writes the .txt training and testing data into binary TF_Records.'''\n",
    "\n",
    "    SAMPLES_PER_FILES=100\n",
    "    print (\"started\")\n",
    "    training_set, test_set=_get_dataset(\"C:\\\\Users\\\\nambi\\\\Desktop\\\\CF\\\\Project\")\n",
    "\n",
    "    for data_set, name, dir_ in zip([training_set, test_set], ['train', 'test'], [\"C:\\\\Users\\\\nambi\\\\Desktop\\\\CF\\\\Project\\\\train\", \"C:\\\\Users\\\\nambi\\\\Desktop\\\\CF\\\\Project\\\\test\"]):\n",
    "        \n",
    "        num_samples=len(data_set)\n",
    "        print (num_samples)\n",
    "        i = 0\n",
    "        fidx = 1\n",
    "\n",
    "        while i < num_samples:\n",
    "           \n",
    "            tf_filename = _get_output_filename(dir_, fidx,  name=name)\n",
    "            print (tf_filename)\n",
    "            \n",
    "            with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n",
    "                \n",
    "                j = 0\n",
    "                \n",
    "                while i < num_samples and j < SAMPLES_PER_FILES:\n",
    "                    \n",
    "                    sys.stdout.write('\\r>> Converting sample %d/%d' % (i+1, num_samples))\n",
    "                    sys.stdout.flush()\n",
    "    \n",
    "                    sample = data_set[i]\n",
    "                    _add_to_tfrecord(sample, tfrecord_writer)\n",
    "                    \n",
    "                    i += 1\n",
    "                    j += 1\n",
    "                fidx += 1\n",
    "\n",
    "    print('\\nFinished converting the dataset!')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #main(output_dir=[OUTPUT_DIR_TRAIN,OUTPUT_DIR_TEST])\n",
    "    main()\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "training_set = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(data, num_users, num_movies):\n",
    "    ''' Making a User-Movie-Matrix'''\n",
    "    \n",
    "    new_data=[]\n",
    "    \n",
    "    for id_user in range(1, num_users+1):\n",
    "        \n",
    "        id_movie=data[:,1][data[:,0]==id_user]\n",
    "        id_rating=data[:,2][data[:,0]==id_user]\n",
    "#         print (id_movie)\n",
    "#         print (id_rating)\n",
    "        ratings=np.zeros(num_movies, dtype=np.uint32)\n",
    "        ratings[id_movie-1]=id_rating\n",
    "        if sum(ratings)==0:\n",
    "            continue\n",
    "        new_data.append(ratings)\n",
    "\n",
    "        del id_movie\n",
    "        del id_rating\n",
    "        del ratings\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def get_dataset_1M(ROOT_DIR):\n",
    "    ''' For each train.dat and test.dat making a User-Movie-Matrix'''\n",
    "    \n",
    "    gc.enable()\n",
    "    print (\"enabled\")\n",
    "    global training_set\n",
    "    \n",
    "    training_set=pd.read_csv(ROOT_DIR+'\\\\train.dat', sep=',', header=None, engine='python', encoding='latin-1')\n",
    "    training_set=np.array(training_set, dtype=np.uint32)\n",
    "    \n",
    "    test_set=pd.read_csv(ROOT_DIR+'\\\\test.dat', sep=',', header=None, engine='python', encoding='latin-1')\n",
    "    test_set=np.array(test_set, dtype=np.uint32)\n",
    "     \n",
    "    num_users=int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "    num_movies=int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "    print (num_users)\n",
    "    print (num_movies)\n",
    "    \n",
    "    training_set=convert(training_set,num_users, num_movies)\n",
    "    test_set=convert(test_set,num_users, num_movies)\n",
    "    print (\"converted\")\n",
    "    \n",
    "    return training_set, test_set\n",
    "    \n",
    "\n",
    "\n",
    "def _get_dataset(ROOT_DIR):\n",
    "    print (\"am I even reachng?\")\n",
    "    return get_dataset_1M(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
